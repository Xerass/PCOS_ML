{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3699828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jomar\\VSCode\\ML\\BasicML\\ML\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random, json\n",
    "import numpy as np\n",
    "import torch, os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, confusion_matrix, classification_report\n",
    "\n",
    "try:\n",
    "    from pytorch_grad_cam import GradCAM\n",
    "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "    _HAS_GRADCAM = True\n",
    "except Exception:\n",
    "    _HAS_GRADCAM = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded21e77",
   "metadata": {},
   "source": [
    "Create Config metrics for easier variable changing later down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f32f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"data\")   # <-- set path\n",
    "TRAIN_DIR, TEST_DIR = DATA_ROOT/\"train\", DATA_ROOT/\"test\"\n",
    "MODEL_DIR = Path(\"models_ultrasound\"); MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BACKBONE = \"efficientnet_b0\"\n",
    "IMG_SIZE, BATCH_SIZE = 224, 12\n",
    "NUM_EPOCHS, FREEZE_EPOCHS = 30, 5\n",
    "LR_HEAD, LR_BACKBONE, WEIGHT_DECAY = 1e-3, 1e-4, 1e-5\n",
    "PATIENCE_ES, VAL_SPLIT, RANDOM_SEED = 6, 0.15, 42\n",
    "POS_LABEL_IDX, NUM_WORKERS = 0, 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b822bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reproduction purposes set all variable random seeds\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d118c",
   "metadata": {},
   "source": [
    "Data transformations, to further augment the data and be better prepared for actual traning and generalization towards messy irl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288b5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since expected input are grayscale (ultrasounds) reproduce it into RGB channels so it can work on pretrained models\n",
    "class Ensure3Channel:\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        return img.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a1c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary transforms for more generalized to prevent overfitting and be better suited to tackle real world input\n",
    "#actual values is very small so it should still be similar to original image\n",
    "train_tf = transforms.Compose([\n",
    "    Ensure3Channel(),\n",
    "    #randomly crops parts of the images\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.80, 1.0)),\n",
    "    #applies a small rotation\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05), shear=5),\n",
    "    #flips image left to right 50% of the time\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    #adds a little color jitter\n",
    "    transforms.ColorJitter(brightness=0.10, contrast=0.10),\n",
    "    #adds a slight blur to simulate low res scans\n",
    "    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    #normalizes the pixels for the expected ImageNet Model\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]), \n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    Ensure3Channel(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b198f4",
   "metadata": {},
   "source": [
    "Create the data loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd19d4e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     35\u001b[39m     test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     36\u001b[39m                              num_workers=NUM_WORKERS, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loader, val_loader, test_loader, class_names, class_counts\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m train_loader, val_loader, test_loader, class_names, class_counts = \u001b[43mbuild_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEST_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClasses:\u001b[39m\u001b[33m\"\u001b[39m, class_names, \u001b[33m\"\u001b[39m\u001b[33mTrain counts:\u001b[39m\u001b[33m\"\u001b[39m, class_counts)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mbuild_dataloaders\u001b[39m\u001b[34m(train_dir, test_dir)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_dataloaders\u001b[39m(train_dir, test_dir):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m#really convenient for subfolder since it expects already that sub dirs are the classes.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     train_full = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_tf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     class_names = train_full.classes\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m#get total length of dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jomar\\VSCode\\ML\\BasicML\\ML\\Lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[39m, in \u001b[36mImageFolder.__init__\u001b[39m\u001b[34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    320\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    321\u001b[39m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m     allow_empty: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    327\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mself\u001b[39m.imgs = \u001b[38;5;28mself\u001b[39m.samples\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jomar\\VSCode\\ML\\BasicML\\ML\\Lib\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[39m, in \u001b[36mDatasetFolder.__init__\u001b[39m\u001b[34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    140\u001b[39m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m   (...)\u001b[39m\u001b[32m    146\u001b[39m     allow_empty: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    147\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(root, transform=transform, target_transform=target_transform)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     classes, class_to_idx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     samples = \u001b[38;5;28mself\u001b[39m.make_dataset(\n\u001b[32m    151\u001b[39m         \u001b[38;5;28mself\u001b[39m.root,\n\u001b[32m    152\u001b[39m         class_to_idx=class_to_idx,\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m         allow_empty=allow_empty,\n\u001b[32m    156\u001b[39m     )\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m.loader = loader\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jomar\\VSCode\\ML\\BasicML\\ML\\Lib\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[39m, in \u001b[36mDatasetFolder.find_classes\u001b[39m\u001b[34m(self, directory)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) -> Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m    208\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[32m    209\u001b[39m \n\u001b[32m    210\u001b[39m \u001b[33;03m        directory/\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m \u001b[33;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jomar\\VSCode\\ML\\BasicML\\ML\\Lib\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[39m, in \u001b[36mfind_classes\u001b[39m\u001b[34m(directory)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) -> Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     classes = \u001b[38;5;28msorted\u001b[39m(entry.name \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry.is_dir())\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'data\\\\train'"
     ]
    }
   ],
   "source": [
    "def build_dataloaders(train_dir, test_dir):\n",
    "    #really convenient for subfolder since it expects already that sub dirs are the classes.\n",
    "    train_full = datasets.ImageFolder(str(train_dir),transform = train_tf)\n",
    "    class_names = train_full.classes\n",
    "\n",
    "    #get total length of dataset\n",
    "    n = len(train_full)\n",
    "    #get indices and shuffle the images around\n",
    "    idx = list(range(n)); random.shuffle(idx)\n",
    "    #create size of val via split\n",
    "    val_n = int(n * VAL_SPLIT)\n",
    "    #slice the respective train val splits\n",
    "    val_idx, tr_idx = idx[:val_n], idx[val_n:]\n",
    "\n",
    "    #generate subsets for each\n",
    "    train_subset = Subset(train_full, tr_idx)\n",
    "    #create the imagefolder with the transform beforehand for val\n",
    "    train_val = datasets.ImageFolder(str(train_dir), transform=val_tf)\n",
    "    val_subset = Subset(train_val, val_idx)\n",
    "\n",
    "    tr_labels = [train_full.samples[i][1] for i in tr_idx]\n",
    "    class_counts = {class_names[i]: tr_labels.count(i) for i in range(len(class_names))}\n",
    "\n",
    "    #from looking at the data there seems to be a ~7:11 ratio for infected vs non, use a sampler to remedy the imbalance\n",
    "    sampler = None\n",
    "    if all(c > 0 for c in class_counts.values()):\n",
    "        weights = np.array([1.0 / class_counts[class_names[y]] for y in tr_labels], dtype=np.float32)\n",
    "        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=(sampler is None),\n",
    "                              sampler=sampler, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    test_ds = datasets.ImageFolder(str(test_dir), transform=val_tf)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                             num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_names, class_counts\n",
    "\n",
    "train_loader, val_loader, test_loader, class_names, class_counts = build_dataloaders(TRAIN_DIR, TEST_DIR)\n",
    "print(\"Classes:\", class_names, \"Train counts:\", class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6529853",
   "metadata": {},
   "source": [
    "The model we are using is the EfficientNetb0, lightest one in the EfficientNet fam, can upgarde if computing power allows (though it might be overkill)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431ea5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model efficientnet_b0 created on cuda\n"
     ]
    }
   ],
   "source": [
    "def create_model(name = BACKBONE, pretrained = True):\n",
    "    return timm.create_model(name, pretrained = pretrained, num_classes = 1)\n",
    "\n",
    "#move model to device\n",
    "model = create_model().to(DEVICE)\n",
    "print(f\"Model {BACKBONE} created on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dcc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since there is a minor class imbalance, we need to punish the model for miscalssifying infected sets\n",
    "def pos_weight_from_counts(counts):\n",
    "    vals = list(counts.values())\n",
    "    n_pos, n_neg = max(1, vals[POS_LABEL_IDX]), max(1, vals[1 - POS_LABEL_IDX])\n",
    "    return torch.tensor([n_neg / n_pos], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "#define the func to train over one epoch (so we can call it in fit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
